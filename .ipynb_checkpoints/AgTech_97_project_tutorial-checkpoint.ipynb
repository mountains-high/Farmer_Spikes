{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "47d5313e-c29d-4581-a9c7-a45122337069",
      "metadata": {
        "id": "47d5313e-c29d-4581-a9c7-a45122337069"
      },
      "source": [
        "[<img src='https://github.com/jeshraghian/snntorch/blob/master/docs/_static/img/snntorch_alpha_w.png?raw=true' width=\"300\">](https://github.com/jeshraghian/snntorch/)\n",
        "\n",
        "# Predicting PV panel power output from weather data with SNNs\n",
        "### From Forecast to Field: Leveraging Spiking Neural Networks for Solar Energy Prediction in Agriculture\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/jeshraghian/snntorch/blob/master/examples/quickstart.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "[<img src='https://github.com/jeshraghian/snntorch/blob/master/docs/_static/img/GitHub-Mark-Light-120px-plus.png?raw=true' width=\"28\">](https://github.com/jeshraghian/snntorch/) [<img src='https://github.com/jeshraghian/snntorch/blob/master/docs/_static/img/GitHub_Logo_White.png?raw=true' width=\"80\">](https://github.com/jeshraghian/snntorch/)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oll2NNFeG1NG",
      "metadata": {
        "id": "oll2NNFeG1NG"
      },
      "source": [
        "For a comprehensive overview on how SNNs work, and what is going on under the hood, [then you might be interested in the snnTorch tutorial series available here.](https://snntorch.readthedocs.io/en/latest/tutorials/index.html)\n",
        "The snnTorch tutorial series is based on the following paper. If you find these resources or code useful in your work, please consider citing the following source:\n",
        "\n",
        "> <cite> [Jason K. Eshraghian, Max Ward, Emre Neftci, Xinxin Wang, Gregor Lenz, Girish Dwivedi, Mohammed Bennamoun, Doo Seok Jeong, and Wei D. Lu. \"Training Spiking Neural Networks Using Lessons From Deep Learning\". Proceedings of the IEEE, 111(9) September 2023.](https://ieeexplore.ieee.org/abstract/document/10242251) </cite>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPt0-TDHsKEd",
        "outputId": "20e72500-3faf-4195-9765-4ab8e452214e"
      },
      "id": "tPt0-TDHsKEd",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hDnIEHOKB8LD",
      "metadata": {
        "id": "hDnIEHOKB8LD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc619d51-74b0-4379-f2a9-0eb193e83772"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WL487gZW1Agy",
      "metadata": {
        "id": "WL487gZW1Agy",
        "outputId": "4f2395d7-64c4-4078-89f5-a47ee4d1b04b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'snntorch'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-e154ae40d84d>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msnntorch\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msnntorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mSF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msnntorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msurrogate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'snntorch'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import os\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "import snntorch as snn\n",
        "import snntorch.functional as SF\n",
        "from snntorch import surrogate\n",
        "from snntorch import utils"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "G7J9h81hyq6o"
      },
      "id": "G7J9h81hyq6o",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y02LeJX7xlyT"
      },
      "outputs": [],
      "source": [
        "# use GPU if available\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ],
      "id": "Y02LeJX7xlyT"
    },
    {
      "cell_type": "markdown",
      "id": "EYf13Gtx1OCj",
      "metadata": {
        "id": "EYf13Gtx1OCj"
      },
      "source": [
        "## 1. Weather dataset\n",
        "\n",
        "We are using weather data obtained from the [NREL System Advisor Model (SAM)](https://sam.nrel.gov/) application. It outputs `.csv` data aggregated by year for the specified location. We have chosen weather data at a location near [Des Moines, Iowa](https://www.google.com/maps/place/42%C2%B001'16.5%22N+93%C2%B046'25.4%22W/@42.0212449,-93.7762968,17z/data=!3m1!4b1!4m4!3m3!8m2!3d42.0212449!4d-93.7737219?entry=ttu). This weather data is used in conjunection with the `nrel-pysam` python module to model the power produced by a theoretical solar array. This data is used in-place of real power output data. The `nrel-pysam` module does not have the capability to estimate power into the future, which is the goal of this tutorial. Below covers the dataloading and preprocessing before being fed into the Neural Nets.\n",
        "\n",
        "### 1.1 Dataloading\n",
        "Define parameters for dataloading. Columns for `Month` and `Hour` are included to account for different seasons in the year and daylight hours respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eo4T5MC21hgD",
      "metadata": {
        "id": "eo4T5MC21hgD"
      },
      "outputs": [],
      "source": [
        "# path to raw weather data\n",
        "DATA_PATH = \"./data/weather_rawdata/\"\n",
        "# Near Des Moines, Iowa\n",
        "LOCATION_PREFIX = \"42.02124491636418_-93.77372190333062_42.0212_-93.7741_psm3_60_\"\n",
        "\n",
        "INPUT_COLUMNS = [\"Month\", \"Hour\", \"DNI\", \"DHI\", \"GHI\", \"Dew Point\", \"Temperature\", \"Pressure\",\n",
        "                 \"Relative Humidity\", \"Wind Direction\", \"Wind Speed\", \"Surface Albedo\",]\n",
        "OUTPUT_COLUMNS = [\"Power Next\"]\n",
        "\n",
        "# number of training data points\n",
        "N_TRAIN_HOURS = 365 * 24 * 18\n",
        "# number of validation data points\n",
        "N_VAL_HOURS = 365 * 24 * 2\n",
        "\n",
        "# list of all filepaths\n",
        "filepaths = []\n",
        "for year in range(2000, 2021):\n",
        "  path = os.path.join(DATA_PATH, f\"{LOCATION_PREFIX}{str(year)}.csv\")\n",
        "  filepaths.append(path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "myFKqNx11qYS",
      "metadata": {
        "id": "myFKqNx11qYS"
      },
      "source": [
        "Load weather dataset into a `pandas.DataFrame`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OeDRqIT0xlyU"
      },
      "outputs": [],
      "source": [
        "weather_data = []\n",
        "\n",
        "for path in filepaths:\n",
        "  weather_data_year = pd.read_csv(path, skiprows=2)\n",
        "  weather_data.append(weather_data_year)\n",
        "\n",
        "weather_data = pd.concat(weather_data)\n",
        "weather_data = weather_data.reset_index(drop=True)\n",
        "\n",
        "# Add timestamp\n",
        "weather_data[\"Timestamp\"] = pd.to_datetime(weather_data[[\"Year\", \"Month\", \"Day\", \"Hour\", \"Minute\"]])\n",
        "weather_data = weather_data.drop([\"Year\", \"Day\", \"Minute\"], axis=1)\n",
        "# reorder columns\n",
        "weather_data = weather_data[[\"Timestamp\", \"Month\", \"Hour\", \"DNI\", \"DHI\", \"GHI\", \"Dew Point\", \"Temperature\", \"Pressure\",\n",
        "                             \"Relative Humidity\", \"Wind Direction\", \"Wind Speed\", \"Surface Albedo\",]]"
      ],
      "id": "OeDRqIT0xlyU"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDr0D41jxlyU"
      },
      "source": [
        "Model power output of PV panels at current timestep of weather data. After modeling, shift the power data back one timestep so that the power data corresponds to the next days power output."
      ],
      "id": "uDr0D41jxlyU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHCq6vvpxlyU"
      },
      "outputs": [],
      "source": [
        "import PySAM.Pvwattsv8 as pv\n",
        "import PySAM.Grid as gr\n",
        "import PySAM.Utilityrate5 as ur\n",
        "import PySAM.Singleowner as so\n",
        "\n",
        "output_power = []\n",
        "\n",
        "for path in tqdm(filepaths):\n",
        "  # create an instance of the Pvwattsv8 module with defaults from the PVWatts - Single Owner configuration\n",
        "  system_model = pv.default('PVWattsSingleOwner')\n",
        "  # create instances of the other modules with shared data from the PVwattsv8 module\n",
        "  grid_model = gr.from_existing(system_model, 'PVWattsSingleOwner')\n",
        "  utilityrate_model = ur.from_existing(system_model, 'PVWattsSingleOwner')\n",
        "  financial_model = so.from_existing(system_model, 'PVWattsSingleOwner')\n",
        "  system_model.SolarResource.solar_resource_file = path\n",
        "  # run the modules in the correct order\n",
        "  system_model.execute()\n",
        "  grid_model.execute()\n",
        "  utilityrate_model.execute()\n",
        "  financial_model.execute()\n",
        "  # display results\n",
        "  #print( 'Annual AC Output in Year 1 = {:,.3f} kWh'.format( system_model.Outputs.ac_annual ) )\n",
        "  #print( 'Net Present Value = ${:,.2f}'.format(financial_model.Outputs.project_return_aftertax_npv) )\n",
        "  dc = system_model.Outputs.dc\n",
        "  #ac = system_model.Outputs.ac\n",
        "\n",
        "  output_power.append(dc)\n",
        "\n",
        "# concat individual lists\n",
        "output_power = np.concatenate(output_power)\n",
        "# create dataframe\n",
        "output_power = pd.DataFrame(output_power, columns=[\"Power\"])\n",
        "# shift to align current weather data to power of the next day\n",
        "output_power[\"Power Next\"] = output_power[\"Power\"].shift(-1)"
      ],
      "id": "DHCq6vvpxlyU"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVH3h0lyxlyV"
      },
      "source": [
        "Examples tables and plots showing the first two months of data. Notice the power output is extremely high, on the order of 100s to 1000s of kWs. When training, the scale of these values posed challenges during training."
      ],
      "id": "iVH3h0lyxlyV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6vyTlEmTxlyV"
      },
      "outputs": [],
      "source": [
        "combined_data = pd.concat([weather_data, output_power], axis=1)\n",
        "combined_data = combined_data.dropna()\n",
        "combined_data = combined_data.set_index(\"Timestamp\")\n",
        "display(combined_data.head(10))\n",
        "\n",
        "first_year = combined_data[combined_data.index < pd.to_datetime(\"2000-02-01\")]\n",
        "first_year.plot(\n",
        "  subplots=True,\n",
        "  title=\"PV Panel Weather and Power Data\",\n",
        "  grid=True,\n",
        "  layout=(7,2),\n",
        "  figsize=(10,12)\n",
        "  )\n",
        "plt.show()"
      ],
      "id": "6vyTlEmTxlyV"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gza7mqHOxlyV"
      },
      "source": [
        "Convert data into train, validation, and test sets and load them onto the GPU. The inputs **AND** outputs are scaled using `scipy.preprocessing.StandardScalar` to create a dataset with `mean = 0` and `std = 1`. Scaling the input feature to the same distribution assigns them equal weights so that features with high magnitudes don't overpower other features. The outputs were scaled as well to work better during NN training."
      ],
      "id": "Gza7mqHOxlyV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0kb-NaxSxlyV"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 72\n",
        "\n",
        "# convert to numpy arrays\n",
        "input_series = combined_data[INPUT_COLUMNS].to_numpy()\n",
        "output_series = combined_data[OUTPUT_COLUMNS].to_numpy()\n",
        "\n",
        "input_scaler = StandardScaler()\n",
        "output_scaler = StandardScaler()\n",
        "\n",
        "input_series = input_scaler.fit_transform(input_series)\n",
        "output_series = output_scaler.fit_transform(output_series)\n",
        "\n",
        "input_series = torch.as_tensor(input_series).to(torch.float32).to(device)\n",
        "output_series = torch.as_tensor(output_series).to(torch.float32).to(device)\n",
        "\n",
        "# training data\n",
        "train_inputs = input_series[:N_TRAIN_HOURS, :]\n",
        "train_outputs = output_series[:N_TRAIN_HOURS]\n",
        "\n",
        "# create dataloaders\n",
        "train_dataset = TensorDataset(train_inputs, train_outputs)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "# validation data\n",
        "val_inputs = input_series[N_TRAIN_HOURS:N_TRAIN_HOURS+N_VAL_HOURS, :]\n",
        "val_outputs = output_series[N_TRAIN_HOURS:N_TRAIN_HOURS+N_VAL_HOURS]\n",
        "\n",
        "val_dataset = TensorDataset(val_inputs, val_outputs)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "# test data\n",
        "test_inputs = input_series[N_TRAIN_HOURS+N_VAL_HOURS:, :]\n",
        "test_outputs = output_series[N_TRAIN_HOURS+N_VAL_HOURS:]\n",
        "\n",
        "test_dataset = TensorDataset(test_inputs, test_outputs)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
      ],
      "id": "0kb-NaxSxlyV"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HamU7lupxlyV"
      },
      "source": [
        "## 2. LSTM Model\n",
        "\n",
        "We first train a simple LSTM network with a single layer of `size = 50` to use as a comparison to the SNN. We use *mean absolute error* as our loss function. On the first attempt the NN was trained using the true output data with really high magnitude resulting in high loss values on the order of `1e6` and slow convergence. As a result we set `LR = 1000` which gave slightly better results. To improve the model, we added output scaling to normalize the data to reasonable values with the results being shown below. We think the better performance was due to being more compatable with the internal parameters within `pytorch`."
      ],
      "id": "HamU7lupxlyV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tn_MN0gExlyV"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# Create network\n",
        "#\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "  def __init__(self, input_size):\n",
        "    \"\"\"LSTM NN Constructor\"\"\"\n",
        "    super(LSTM, self).__init__()\n",
        "\n",
        "    self.lstm = nn.LSTM(input_size, 50)\n",
        "    self.fc = nn.Linear(50, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"Forward pass of LSTM network\"\"\"\n",
        "\n",
        "    lstm_out, _ = self.lstm(x)\n",
        "    output = self.fc(lstm_out)\n",
        "    return output\n",
        "\n",
        "# load onto GPU\n",
        "lstm_net = LSTM(len(train_inputs[0])).to(device)\n",
        "\n",
        "\n",
        "#\n",
        "# Train Network\n",
        "#\n",
        "\n",
        "# default learning rate from tf.keras.optimizers.Adam\n",
        "LEARNING_RATE = 0.0001\n",
        "\n",
        "EPOCHS = 100\n",
        "\n",
        "# Adam optimizers\n",
        "optimizer = optim.Adam(lstm_net.parameters(), lr=LEARNING_RATE)\n",
        "# Mean absolute error\n",
        "loss_fn = nn.L1Loss()\n",
        "\n",
        "\n",
        "loss_train_hist = []\n",
        "loss_val_hist = []\n",
        "\n",
        "\n",
        "# training loop\n",
        "for epoch in tqdm(range(EPOCHS)):\n",
        "  loss_train_epoch = []\n",
        "\n",
        "  lstm_net.train()\n",
        "  for inputs, outputs in train_loader:\n",
        "    # forward pass\n",
        "    predictions = lstm_net(inputs)\n",
        "    # calculate loss from membrane potential at last timestep\n",
        "    loss_val = loss_fn(predictions, outputs)\n",
        "    # zero out gradients\n",
        "    optimizer.zero_grad()\n",
        "    # calculate gradients\n",
        "    loss_val.backward()\n",
        "    # update weights\n",
        "    optimizer.step()\n",
        "\n",
        "    # store loss\n",
        "    loss_train_epoch.append(loss_val.item())\n",
        "\n",
        "  # calculate average loss p/epoch\n",
        "  avg_loss_epoch_train = sum(loss_train_epoch) / len(loss_train_epoch)\n",
        "  loss_train_hist.append(avg_loss_epoch_train)\n",
        "\n",
        "\n",
        "  loss_val_epoch = []\n",
        "\n",
        "  lstm_net.eval()\n",
        "  for inputs, outputs in val_loader:\n",
        "    predictions = lstm_net(inputs)\n",
        "    loss_val = loss_fn(predictions, outputs)\n",
        "    loss_val_epoch.append(loss_val.item())\n",
        "\n",
        "  avg_loss_epoch_val = sum(loss_val_epoch) / len(loss_val_epoch)\n",
        "  loss_val_hist.append(avg_loss_epoch_val)\n",
        "\n",
        "  print(f\"Epoch: {epoch+1}/{EPOCHS}, Train Loss: {avg_loss_epoch_train}, Val Loss: {avg_loss_epoch_val}\")"
      ],
      "id": "tn_MN0gExlyV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHlcOZqUxlyV"
      },
      "outputs": [],
      "source": [
        "# Plot of train and loss functions\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.subplot(2,1,1)\n",
        "plt.plot(loss_train_hist)\n",
        "plt.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\n",
        "plt.ylabel(\"Training Loss\")\n",
        "plt.title(\"Performance Metrics\")\n",
        "plt.grid()\n",
        "\n",
        "plt.subplot(2,1,2)\n",
        "plt.plot(loss_val_hist)\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Validation Loss\")\n",
        "plt.grid()\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ],
      "id": "nHlcOZqUxlyV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vj9MptjkxlyW"
      },
      "outputs": [],
      "source": [
        "# Run on test data\n",
        "\n",
        "lstm_net.eval()\n",
        "with torch.no_grad():\n",
        "  predictions = lstm_net(test_inputs)\n",
        "\n",
        "#predictions = predictions.cpu()\n",
        "#predictions = output_scaler.inverse_transform(predictions)\n",
        "\n",
        "fix, ax = plt.subplots()\n",
        "\n",
        "ax.plot(output_scaler.inverse_transform(test_outputs.cpu()[0:720]), \"--\", label=\"Actual\")\n",
        "ax.plot(output_scaler.inverse_transform(predictions.cpu()[0:720]), label=\"Predicted\")\n",
        "\n",
        "ax.set_xlabel(\"Hours\")\n",
        "ax.set_ylabel(\"Power Output (W)\")\n",
        "\n",
        "ax.legend()\n",
        "ax.grid()\n",
        "\n",
        "plt.show()"
      ],
      "id": "vj9MptjkxlyW"
    },
    {
      "cell_type": "markdown",
      "id": "BtJBOtez11wy",
      "metadata": {
        "id": "BtJBOtez11wy"
      },
      "source": [
        "## 3. Spiking Neural Network (SNN)\n",
        "\n",
        "The SNN is now trained. We use 3-layers of leaky integrate-and-fire neurons with `size=128`. Notably, the last layer has the reset mechanism disabled to give us a single value output of membrane potential rather than the typical output spikes. The input features are passed directly into the network and repeated `TIMESTEP=20` times. The training of the SNN takes significantly more time than the LSTM network (~3hr with a Nvidia 1080Ti)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JM2thnrc10rD",
      "metadata": {
        "id": "JM2thnrc10rD"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# Define model\n",
        "#\n",
        "class SNN(torch.nn.Module):\n",
        "    \"\"\"Simple spiking neural network in snntorch.\"\"\"\n",
        "\n",
        "    def __init__(self, timesteps, in_features, hidden):\n",
        "        super().__init__()\n",
        "\n",
        "        self.timesteps = timesteps # number of time steps to simulate the network\n",
        "        self.hidden = hidden # number of hidden neurons\n",
        "        spike_grad = surrogate.fast_sigmoid() # surrogate gradient function\n",
        "\n",
        "        # randomly initialize decay rate and threshold for layer 1\n",
        "        beta_in = torch.rand(self.hidden)\n",
        "        thr_in = torch.rand(self.hidden)\n",
        "\n",
        "        # layer 1\n",
        "        self.fc_in = torch.nn.Linear(in_features=in_features, out_features=self.hidden)\n",
        "        self.lif_in = snn.Leaky(beta=beta_in, threshold=thr_in, learn_beta=True, spike_grad=spike_grad)\n",
        "\n",
        "        # randomly initialize decay rate and threshold for layer 2\n",
        "        beta_hidden = torch.rand(self.hidden)\n",
        "        thr_hidden = torch.rand(self.hidden)\n",
        "\n",
        "        # layer 2\n",
        "        self.fc_hidden = torch.nn.Linear(in_features=self.hidden, out_features=self.hidden)\n",
        "        self.lif_hidden = snn.Leaky(beta=beta_hidden, threshold=thr_hidden, learn_beta=True, spike_grad=spike_grad)\n",
        "\n",
        "        # randomly initialize decay rate for output neuron\n",
        "        beta_out = torch.rand(1)\n",
        "\n",
        "        # layer 3: leaky integrator neuron. Note the reset mechanism is disabled and we will disregard output spikes.\n",
        "        self.fc_out = torch.nn.Linear(in_features=self.hidden, out_features=1)\n",
        "        self.li_out = snn.Leaky(beta=beta_out, threshold=1.0, learn_beta=True, spike_grad=spike_grad, reset_mechanism=\"none\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass for several time steps.\"\"\"\n",
        "\n",
        "        # Initalize membrane potential\n",
        "        mem_1 = self.lif_in.init_leaky()\n",
        "        mem_2 = self.lif_hidden.init_leaky()\n",
        "        mem_3 = self.li_out.init_leaky()\n",
        "\n",
        "        # Empty lists to record outputs\n",
        "        mem_3_rec = []\n",
        "\n",
        "        # Loop over\n",
        "        for step in range(self.timesteps):\n",
        "            cur_in = self.fc_in(x)\n",
        "            spk_in, mem_1 = self.lif_in(cur_in, mem_1)\n",
        "\n",
        "            cur_hidden = self.fc_hidden(spk_in)\n",
        "            spk_hidden, mem_2 = self.lif_hidden(cur_hidden, mem_2)\n",
        "\n",
        "            cur_out = self.fc_out(spk_hidden)\n",
        "            _, mem_3 = self.li_out(cur_out, mem_3)\n",
        "\n",
        "            mem_3_rec.append(mem_3)\n",
        "\n",
        "        return torch.stack(mem_3_rec)\n",
        "\n",
        "# Parameters\n",
        "TIMESTEPS = 20\n",
        "HIDDEN_LAYERS = 128\n",
        "\n",
        "snn_net = SNN(\n",
        "  timesteps=TIMESTEPS,\n",
        "  hidden=HIDDEN_LAYERS,\n",
        "  in_features=len(train_inputs[0])\n",
        "  )\n",
        "snn_net = snn_net.to(device)\n",
        "\n",
        "\n",
        "#\n",
        "# Output without any training\n",
        "#\n",
        "\n",
        "# run a single forward-pass to see what output is\n",
        "with torch.no_grad():\n",
        "  for inputs, outputs in train_loader:\n",
        "    mem = snn_net(inputs)\n",
        "\n",
        "# record outputs for later plotting\n",
        "sample_outputs = outputs\n",
        "sample_mem = mem\n",
        "\n",
        "\n",
        "#\n",
        "# Training loop\n",
        "#\n",
        "\n",
        "# hyperparameters\n",
        "EPOCHS = 100\n",
        "LEARNING_RATE = 1e-3\n",
        "\n",
        "# optimizer\n",
        "optimizer = torch.optim.Adam(params=snn_net.parameters(), lr=1e-3)\n",
        "# loss function for membrane potential\n",
        "loss_function = torch.nn.MSELoss()\n",
        "\n",
        "# list to store loss at each timestep\n",
        "loss_train_hist = []\n",
        "loss_val_hist = []\n",
        "\n",
        "# training loop\n",
        "for epoch in tqdm(range(EPOCHS)):\n",
        "  loss_train_epoch = []\n",
        "\n",
        "  snn_net.train()\n",
        "  for inputs, outputs in train_loader:\n",
        "    # forward pass\n",
        "    mem = snn_net(inputs)\n",
        "    # calculate loss from membrane potential at last timestep\n",
        "    loss_val = loss_function(mem[-1,:,:], outputs)\n",
        "    # zero out gradients\n",
        "    optimizer.zero_grad()\n",
        "    # calculate gradients\n",
        "    loss_val.backward()\n",
        "    # update weights\n",
        "    optimizer.step()\n",
        "\n",
        "    # store loss\n",
        "    loss_train_epoch.append(loss_val.item())\n",
        "\n",
        "  # calculate average loss p/epoch\n",
        "  avg_loss_epoch_train = sum(loss_train_epoch) / len(loss_train_epoch)\n",
        "  loss_train_hist.append(avg_loss_epoch_train)\n",
        "\n",
        "\n",
        "  loss_val_epoch = []\n",
        "\n",
        "  snn_net.eval()\n",
        "  for inputs, outputs in val_loader:\n",
        "    mem = snn_net(inputs)\n",
        "    loss_val = loss_function(mem[-1,:,:], outputs)\n",
        "    loss_val_epoch.append(loss_val.item())\n",
        "\n",
        "  avg_loss_epoch_val = sum(loss_val_epoch) / len(loss_val_epoch)\n",
        "  loss_val_hist.append(avg_loss_epoch_val)\n",
        "\n",
        "  print(f\"Epoch: {epoch+1}/{EPOCHS}, Train Loss: {avg_loss_epoch_train}, Val Loss: {avg_loss_epoch_val}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ckZqeiixlyW"
      },
      "outputs": [],
      "source": [
        "# Plot of network before training\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "# plot expected output\n",
        "ax.plot(sample_outputs.squeeze(1).cpu(), '--', label=\"Target\")\n",
        "# plot first 5 membrane potential outputs\n",
        "for idx in range(0, min(TIMESTEPS, 5)):\n",
        "  ax.plot(sample_mem[idx,:,0].cpu(), alpha=0.6)\n",
        "\n",
        "ax.set_title(\"Untrained Output Neuron\")\n",
        "ax.set_xlabel(\"Time\")\n",
        "ax.set_ylabel(\"Membrane Potential\")\n",
        "ax.legend(loc='best')\n",
        "plt.show()"
      ],
      "id": "4ckZqeiixlyW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPjyRvgUxlyW"
      },
      "outputs": [],
      "source": [
        "# Plot of train and loss functions\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.subplot(2,1,1)\n",
        "plt.plot(loss_train_hist)\n",
        "plt.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\n",
        "plt.ylabel(\"Training Loss\")\n",
        "plt.title(\"Performance Metrics\")\n",
        "plt.grid()\n",
        "\n",
        "plt.subplot(2,1,2)\n",
        "plt.plot(loss_val_hist)\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Validation Loss\")\n",
        "plt.grid()\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()\n"
      ],
      "id": "nPjyRvgUxlyW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFscRDCgxlyW"
      },
      "outputs": [],
      "source": [
        "# Run on test data\n",
        "\n",
        "snn_net.eval()\n",
        "with torch.no_grad():\n",
        "  predictions = snn_net(test_inputs)\n",
        "\n",
        "#predictions = predictions.cpu()\n",
        "#predictions = output_scaler.inverse_transform(predictions)\n",
        "\n",
        "fix, ax = plt.subplots()\n",
        "\n",
        "ax.plot(output_scaler.inverse_transform(test_outputs.cpu()[0:720]), \"--\", label=\"Actual\")\n",
        "ax.plot(output_scaler.inverse_transform(predictions.cpu()[0,0:720,:]), label=\"Predicted\", alpha=0.6)\n",
        "\n",
        "ax.set_xlabel(\"Hours\")\n",
        "ax.set_ylabel(\"Power Output (W)\")\n",
        "\n",
        "ax.legend()\n",
        "ax.grid()\n",
        "\n",
        "plt.show()"
      ],
      "id": "OFscRDCgxlyW"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mmL4Z5pxlyW"
      },
      "source": [
        "## Comparison Between LSTM and SNN\n",
        "\n",
        "Below shows the loss function (mean absolute error) for the test dataset after being transformed back into the original units. The SNN had approximately double the error as the LSTM when trained under similar conditions. The plot of SNN output shows it has issues with extreme points of the data when it was close to zero or at peak output. There were less over estimations compared to the LSTM."
      ],
      "id": "_mmL4Z5pxlyW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6pjYe1nxlyW"
      },
      "outputs": [],
      "source": [
        "TW = 24 * 14\n",
        "\n",
        "lstm_net.eval()\n",
        "snn_net.eval()\n",
        "with torch.no_grad():\n",
        "  lstm_predictions = lstm_net(test_inputs)\n",
        "  snn_predictions = snn_net(test_inputs)\n",
        "\n",
        "loss_fn = nn.L1Loss()\n",
        "\n",
        "lstm_predictions = output_scaler.inverse_transform(lstm_predictions.cpu())\n",
        "snn_predictions = output_scaler.inverse_transform(snn_predictions.cpu()[0,:,:])\n",
        "actual_output = output_scaler.inverse_transform(test_outputs.cpu())\n",
        "\n",
        "mae_lstm = np.mean(np.abs(actual_output - lstm_predictions))\n",
        "mae_snn = np.mean(np.abs(actual_output - snn_predictions))\n",
        "\n",
        "print(f\"LSTM MAE: {mae_lstm}\")\n",
        "print(f\"SNN MAE: {mae_snn}\")\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(actual_output[0:TW], \"--\", label=\"Actual\")\n",
        "plt.plot(lstm_predictions[0:TW], alpha=0.6, label=\"LSTM\")\n",
        "plt.plot(snn_predictions[0:TW], alpha=0.6, label=\"SNN\")\n",
        "\n",
        "plt.xlabel(\"Hours\")\n",
        "plt.ylabel(\"Power (W)\")\n",
        "plt.title(\"Comparison between PV panel power output prediction methods\")\n",
        "\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "\n",
        "plt.show()"
      ],
      "id": "a6pjYe1nxlyW"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "c8b87b4648a8d1ba1118329c37c7c28a2ff48490805f0e62ea19d4b1b49e5656"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}